{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3259d9dafd2d42e3b9d5ed1b94272a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Original (higher quality)",
              "LCM (faster)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Checkpoint:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_1724febf4d17448d878a0b3a8636c45e",
            "style": "IPY_MODEL_5d96d556a5d542d6be5025556c3a7627"
          }
        },
        "1724febf4d17448d878a0b3a8636c45e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d96d556a5d542d6be5025556c3a7627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NwHyTGhSTEu",
        "outputId": "89d7103b-331e-4eed-b465-c022389a81f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import data (stored in google drive)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Checkpoint\n",
        "!pip install ipywidgets==7.7.1 --quiet\n",
        "\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import shutil\n",
        "\n",
        "ckpt_dic = {\n",
        "    \"Original (higher quality)\": \"prs-eth/marigold-v1-0\",\n",
        "    \"LCM (faster)\": \"prs-eth/marigold-lcm-v1-0\",\n",
        "}\n",
        "\n",
        "ckpt_name = 'LCM (faster)'\n",
        "ckpt_path = ckpt_dic[ckpt_name]\n",
        "w = widgets.Dropdown(\n",
        "    options=['Original (higher quality)', 'LCM (faster)'],\n",
        "    value=ckpt_name,\n",
        "    description='Checkpoint:',\n",
        ")\n",
        "\n",
        "\n",
        "def on_change(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        ckpt_name = change['new']\n",
        "        ckpt_path = ckpt_dic[ckpt_name]\n",
        "\n",
        "w.observe(on_change)\n",
        "\n",
        "display(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "3259d9dafd2d42e3b9d5ed1b94272a04",
            "1724febf4d17448d878a0b3a8636c45e",
            "5d96d556a5d542d6be5025556c3a7627"
          ]
        },
        "id": "Mvjs71GxTFxs",
        "outputId": "5f5d6cb5-b58c-4315-f125-e65ba676dd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Checkpoint:', index=1, options=('Original (higher quality)', 'LCM (faster)'), value='LCM…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3259d9dafd2d42e3b9d5ed1b94272a04"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone Marigold repository -- notify Erich about pulling Marigold model\n",
        "%%shell\n",
        "cd /content\n",
        "\n",
        "if [ -d \"Marigold\" ]; then\n",
        "    cd Marigold\n",
        "    git pull\n",
        "else\n",
        "    git clone https://github.com/prs-eth/Marigold.git\n",
        "    cd Marigold\n",
        "fi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvv9eDN_TJbd",
        "outputId": "29182c25-11ad-4c86-81b8-76a803db4fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Marigold'...\n",
            "remote: Enumerating objects: 472, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
            "remote: Total 472 (delta 204), reused 224 (delta 133), pack-reused 151\u001b[K\n",
            "Receiving objects: 100% (472/472), 5.66 MiB | 27.98 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies from Marigold\n",
        "%%shell\n",
        "\n",
        "cd /content/Marigold\n",
        "\n",
        "# pip install -r requirements.txt --upgrade  --quiet\n",
        "pip install accelerate diffusers matplotlib scipy torch transformers --quiet\n",
        "\n",
        "# for progress bar\n",
        "pip install ipywidgets==7.7.1 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKmo2Q4jTPda",
        "outputId": "bcdcc249-ff3f-4d71-d4ac-dd17b27285af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directories\n",
        "import os\n",
        "\n",
        "# Directories\n",
        "repo_dir = \"/content/Marigold\"\n",
        "input_dir = os.path.join(\"/content/drive/MyDrive/sample\", \"Image\")\n",
        "output_dir = os.path.join(repo_dir, \"outputs\")\n",
        "output_dir_color = os.path.join(output_dir, \"depth_colored\")\n",
        "output_dir_tif = os.path.join(output_dir, \"depth_bw\")\n",
        "output_dir_npy = os.path.join(output_dir, \"depth_npy\")\n",
        "\n",
        "os.chdir(repo_dir)"
      ],
      "metadata": {
        "id": "p9JKGq-STSol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from scipy.ndimage import zoom\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "nVtN6V1rTa5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rmse(predictions, ground_truth):\n",
        "    \"\"\"\n",
        "    Calculate RMSE, resizing the prediction to the ground truth dimensions if necessary.\n",
        "    \"\"\"\n",
        "    if predictions.shape != ground_truth.shape:\n",
        "        predictions = resize_depth_map(predictions, ground_truth.shape)\n",
        "    # print(\"Prediction: \", predictions[0:6])\n",
        "    # print(\"Ground truth: \" + ground_truth[0:6])\n",
        "    # return np.sqrt(np.mean((predictions - ground_truth) ** 2))\n",
        "\n",
        "    rms = mean_squared_error(ground_truth, predictions, squared=False)\n",
        "    return rms"
      ],
      "metadata": {
        "id": "iiCkwXkRTcUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_depth_map(source, target_shape):\n",
        "    \"\"\"\n",
        "    Resizes the source array to the target shape using simple resampling,\n",
        "    which is suitable for depth maps where preserving exact pixel values isn't crucial.\n",
        "    \"\"\"\n",
        "    # Calculate the zoom factors for each dimension\n",
        "    zoom_factors = (target_shape[0] / source.shape[0], target_shape[1] / source.shape[1])\n",
        "    return zoom(source, zoom_factors, order=1)  # Using bilinear interpolation (order=1)"
      ],
      "metadata": {
        "id": "45DYUWLdTgXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_depth_maps(buffer):\n",
        "    if len(buffer) > 0:\n",
        "        return np.mean(np.array(buffer), axis=0)\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "U85HKXGgThAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_average_depth_maps(depth_maps, weights):\n",
        "    \"\"\"Calculate the weighted average of depth maps.\"\"\"\n",
        "    if len(depth_maps) != len(weights):\n",
        "        raise ValueError(\"The number of depth maps and weights must match\")\n",
        "    # Stack the depth maps along the third dimension\n",
        "    stacked_depth_maps = np.stack(depth_maps, axis=-1)\n",
        "    # Normalize weights to ensure they sum to 1\n",
        "    normalized_weights = np.array(weights) / np.sum(weights)\n",
        "    # Perform the weighted average along the stack\n",
        "    weighted_avg_depth_map = np.average(stacked_depth_maps, axis=-1, weights=normalized_weights)\n",
        "\n",
        "    return weighted_avg_depth_map"
      ],
      "metadata": {
        "id": "Y4qVBn41TiHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_weights(depth_maps, depth_gt, initial_weights=None, epochs=100, learning_rate=0.01):\n",
        "    depth_maps_tensor = torch.stack([dm.clone().detach() for dm in depth_maps])\n",
        "    depth_gt_tensor = depth_gt.clone().detach()\n",
        "\n",
        "    # Initialize weights or use provided initial weights\n",
        "    if initial_weights is None:\n",
        "        weights = torch.randn(depth_maps_tensor.shape[0], requires_grad=True)\n",
        "    else:\n",
        "        weights = torch.tensor(initial_weights, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam([weights], lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate weighted average of depth maps\n",
        "        expanded_weights = weights.unsqueeze(1).unsqueeze(2)\n",
        "        weighted_depth_maps = torch.mul(expanded_weights, depth_maps_tensor)\n",
        "        weighted_average = torch.sum(weighted_depth_maps, dim=0) / torch.sum(weights)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = torch.mean((weighted_average - depth_gt_tensor) ** 2)\n",
        "\n",
        "        weighted_depth_maps.requires_grad = True\n",
        "        weighted_average.requires_grad = True\n",
        "        loss.requires_grad = True\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # Normalize weights to ensure they sum to 1\n",
        "    with torch.no_grad():\n",
        "        normalized_weights = weights / weights.sum()\n",
        "\n",
        "    return normalized_weights.cpu().numpy()"
      ],
      "metadata": {
        "id": "UE9nNW6GToG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize directories and pipeline for Marigold\n",
        "from marigold import MarigoldPipeline\n",
        "repo_dir = \"/content/Marigold\"\n",
        "ckpt_path = \"prs-eth/marigold-lcm-v1-0\"\n",
        "pipe = MarigoldPipeline.from_pretrained(ckpt_path).to(\"cuda\")"
      ],
      "metadata": {
        "id": "dnyh0RHkTqCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FERqjidBTxfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base directory for training data\n",
        "base_dir = \"/content/drive/MyDrive/data\"\n",
        "# Process each dataset\n",
        "datasets = sorted(glob(os.path.join(base_dir, \"sample_*\")))\n",
        "# Define lists to store paths for each set\n",
        "train_paths = []\n",
        "val_paths = []\n",
        "test_paths = []"
      ],
      "metadata": {
        "id": "h-h_RIqFTtLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_sort_key(s):\n",
        "    \"\"\"\n",
        "    Extracts numbers from a filename and returns them for sorting purposes.\n",
        "    \"\"\"\n",
        "    return [int(text) if text.isdigit() else text for text in re.split('(\\d+)', s)]"
      ],
      "metadata": {
        "id": "gTJTL3N4T1Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(files, test_size, val_size):\n",
        "    \"\"\"\n",
        "    Manually splits the data into training, validation, and test sets without shuffling.\n",
        "    \"\"\"\n",
        "    n = len(files)\n",
        "    test_count = int(n * test_size)\n",
        "    val_count = int(n * val_size)\n",
        "\n",
        "    test_files = files[:test_count]\n",
        "    val_files = files[test_count:test_count + val_count]\n",
        "    train_files = files[test_count + val_count:]\n",
        "\n",
        "    return train_files, val_files, test_files"
      ],
      "metadata": {
        "id": "HmNigR1XT1ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each video folder\n",
        "for video_folder in os.listdir(base_dir):\n",
        "    # Skip directories that do not contain 'sample_'\n",
        "    if \"sample_\" not in video_folder:\n",
        "        continue\n",
        "    video_path = os.path.join(base_dir, video_folder)\n",
        "    if not os.path.isdir(video_path):\n",
        "        continue\n",
        "\n",
        "    # Gather paths to input images and depth maps\n",
        "    image_dir = os.path.join(video_path, \"Image\")\n",
        "    depth_dir = os.path.join(video_path, \"Depth\")\n",
        "\n",
        "    # Sorting files numerically\n",
        "    image_files = sorted(os.listdir(image_dir), key=numerical_sort_key)\n",
        "    depth_files = sorted(os.listdir(depth_dir), key=numerical_sort_key)\n",
        "    assert len(image_files) == len(depth_files), f\"Mismatch in number of files for {video_folder}\"\n",
        "\n",
        "    # Split paths into training, validation, and test sets without shuffling\n",
        "    image_train, image_val, image_test = split_data(image_files, test_size=0.15, val_size=0.1765)\n",
        "    depth_train, depth_val, depth_test = split_data(depth_files, test_size=0.15, val_size=0.1765)\n",
        "\n",
        "    # Store paths for each set\n",
        "    train_paths.extend([(os.path.join(image_dir, img), os.path.join(depth_dir, depth)) for img, depth in zip(image_train, depth_train)])\n",
        "    val_paths.extend([(os.path.join(image_dir, img), os.path.join(depth_dir, depth)) for img, depth in zip(image_val, depth_val)])\n",
        "    test_paths.extend([(os.path.join(image_dir, img), os.path.join(depth_dir, depth)) for img, depth in zip(image_test, depth_test)])\n"
      ],
      "metadata": {
        "id": "KWzz53mET22N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples in each set\n",
        "print(f\"Number of samples in training set: {len(train_paths)}\")\n",
        "print(f\"Number of samples in validation set: {len(val_paths)}\")\n",
        "print(f\"Number of samples in test set: {len(test_paths)}\")"
      ],
      "metadata": {
        "id": "C1-vSemqT6pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import pickle"
      ],
      "metadata": {
        "id": "_kRws5D0T7kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_video_id(filepath):\n",
        "    # Regex finds 'sample_' followed by any digits (\\d+), capturing the digits only\n",
        "    match = re.search(r'sample_(\\d+)', filepath)\n",
        "    if match:\n",
        "        return match.group(1)  # Returns the first group (the digits following 'sample_')\n",
        "    return None  # Return None if no match is found\n"
      ],
      "metadata": {
        "id": "u06mWJ6qT8H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "test_errors = []\n",
        "errors = []\n",
        "datasets = [train_paths[:400], val_paths[:80], test_paths[:80]]   # Assuming these are already defined\n",
        "dataset_names = [\"Training\", \"Validation\", \"Testing\"]\n",
        "current_video_id = None\n",
        "optimized_weights = [1,1,1,1,1,1,1,1,1,1]\n",
        "weights_history = []  # To store weights for each iteration\n",
        "\n",
        "for dataset, name in zip(datasets, dataset_names):\n",
        "    # Create a new figure and axis object for each dataset\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    print(f\"Processing {name} dataset\")\n",
        "    rgb_files = [path[0] for path in dataset]\n",
        "    depth_gt_files = [path[1] for path in dataset]\n",
        "\n",
        "    depth_buffer = deque(maxlen=5)\n",
        "    depth_gt_buffer = deque(maxlen=5)\n",
        "\n",
        "    if name == \"Training\":\n",
        "        errors = train_errors\n",
        "    elif name == \"Validation\":\n",
        "        errors = val_errors\n",
        "    elif name == \"Testing\":\n",
        "        errors = test_errors\n",
        "\n",
        "    for rgb_path, depth_gt_path in tqdm(zip(rgb_files, depth_gt_files), total=len(rgb_files), desc=f\"Processing {name} images\"):\n",
        "        new_video_id = extract_video_id(rgb_path)\n",
        "\n",
        "        # Check if the video ID has changed (new video sequence)\n",
        "        if current_video_id is not None and new_video_id != current_video_id:\n",
        "            # Flush the buffers when a new video sequence is detected\n",
        "            depth_buffer.clear()\n",
        "            depth_gt_buffer.clear()\n",
        "\n",
        "        current_video_id = new_video_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "            depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "\n",
        "            depth_gt_array = np.array(depth_gt_image)\n",
        "            depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "            depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(input_image.size, Image.BILINEAR)\n",
        "            depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "            # Call depth estimation pipeline\n",
        "            pipeline_output = pipe(\n",
        "                input_image,\n",
        "                denoising_steps=4,\n",
        "                ensemble_size=5,\n",
        "                processing_res=768,\n",
        "                match_input_res=True,\n",
        "                color_map=\"Spectral\",\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "            depth_pred = pipeline_output.depth_np\n",
        "\n",
        "            depth_buffer.append(depth_pred)\n",
        "            depth_gt_buffer.append(depth_gt_final)\n",
        "\n",
        "            if len(depth_buffer) > 0:\n",
        "                depth_maps_tensor = torch.stack([torch.tensor(dm, dtype=torch.float32) for dm in depth_buffer])\n",
        "                depth_gt_tensor = torch.tensor(depth_gt_final, dtype=torch.float32)\n",
        "\n",
        "                if name != \"Testing\":\n",
        "                    if len(optimized_weights) != len(depth_buffer):\n",
        "                      optimized_weights = optimize_weights(depth_maps_tensor, depth_gt_tensor)\n",
        "                    else:\n",
        "                      optimized_weights = optimize_weights(depth_maps_tensor, depth_gt_tensor, initial_weights=optimized_weights)\n",
        "                depth_maps_array = np.array(list(depth_buffer))\n",
        "                if optimized_weights is None or len(optimized_weights) != len(depth_buffer):\n",
        "                    # Initialize or reset weights if they don't match the current buffer size\n",
        "                    current_weights = np.ones(len(depth_buffer)) / len(depth_buffer)\n",
        "                else:\n",
        "                    # Ensure optimized_weights is a numpy array with the correct length\n",
        "                    current_weights = optimized_weights[:len(depth_buffer)]\n",
        "\n",
        "                # Compute the weighted average\n",
        "                weighted_depth_pred = np.average(depth_maps_array, axis=0, weights=current_weights)\n",
        "                rmse = calculate_rmse(weighted_depth_pred, depth_gt_final)\n",
        "                errors.append(rmse)\n",
        "                print(f'RMSE for {name}: {rmse}')\n",
        "\n",
        "\n",
        "                weights_history.append(optimized_weights.copy())  # Store a copy of weights for each iteration\n",
        "\n",
        "                # Save errors and weights_history to a file for every new image processed\n",
        "                with open(f\"{name.lower()}_errors_and_weights.pkl\", \"wb\") as f:\n",
        "                    pickle.dump({\"errors\": errors, \"weights_history\": weights_history}, f)\n",
        "\n",
        "\n",
        "        # Update the plot for every iteration\n",
        "        ax.plot(errors, label=f'RMSE per Image for {name} if not ax.lines else \"Update')\n",
        "        ax.set_xlabel('Image Index')\n",
        "        ax.set_ylabel('RMSE')\n",
        "        ax.set_title(f'Running RMSE Across {name} Dataset')\n",
        "        if not ax.lines:\n",
        "            ax.legend()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(fig)\n",
        "        print(\"Current Weights\", optimized_weights)\n",
        "\n",
        "    # Save the graph as a PDF before changing from training to validation to testing\n",
        "    plt.savefig(f\"{name.lower()}_graph.pdf\")\n",
        "\n",
        "    plt.close(fig)\n",
        "    print(optimized_weights)\n",
        "\n",
        "    print(f\"Average RMSE for {name} dataset:\", np.mean(errors))"
      ],
      "metadata": {
        "id": "fXABbzBOT_Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain each video data for testing\n",
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "base_dir = \"/content/drive/MyDrive/data\"\n",
        "\n",
        "# Create a dictionary to store paths for each dataset\n",
        "dataset_paths = {}\n",
        "# Iterate over each video folder\n",
        "for video_folder in os.listdir(base_dir):\n",
        "    # Skip directories that do not contain 'sample_'\n",
        "    if \"sample_\" not in video_folder:\n",
        "        continue\n",
        "    video_path = os.path.join(base_dir, video_folder)\n",
        "    if not os.path.isdir(video_path):\n",
        "        continue\n",
        "\n",
        "    # Gather paths to input images and depth maps\n",
        "    image_dir = os.path.join(video_path, \"Image\")\n",
        "    depth_dir = os.path.join(video_path, \"Depth\")\n",
        "    # print(image_dir, depth_dir)\n",
        "\n",
        "    # Ensure directories exist\n",
        "    if not os.path.exists(image_dir) or not os.path.exists(depth_dir):\n",
        "        continue\n",
        "\n",
        "    # Sorting files numerically\n",
        "    image_files = sorted(os.listdir(image_dir), key=numerical_sort_key)\n",
        "    depth_files = sorted(os.listdir(depth_dir), key=numerical_sort_key)\n",
        "    assert len(image_files) == len(depth_files), f\"Mismatch in number of files for {video_folder}\"\n",
        "\n",
        "    # Store paths for each image-depth pair in the dictionary\n",
        "    dataset_paths[video_folder] = [(os.path.join(image_dir, img), os.path.join(depth_dir, depth)) for img, depth in zip(image_files, depth_files)]"
      ],
      "metadata": {
        "id": "oHJLcdEYX5_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_paths)"
      ],
      "metadata": {
        "id": "yLQjbygSYR7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_errors(errors, title):\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.plot(errors, label='RMSE per Image')\n",
        "    ax.set_xlabel('Image Index')\n",
        "    ax.set_ylabel('RMSE')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "AKwqqnobeNfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_optimized(rgb_depth_pairs):\n",
        "    depth_buffer = deque(maxlen=5)\n",
        "    errors = []\n",
        "    frame_buffer = deque(maxlen=5)\n",
        "    original_weights = np.array([0.2854117, 0.04321782, -0.33442682, 0.27889505, 0.7269023])\n",
        "\n",
        "    for rgb_path, depth_gt_path in tqdm(rgb_depth_pairs, desc=\"Processing images\"):\n",
        "        with torch.no_grad():\n",
        "            input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "            input_array = np.array(input_image, dtype=np.float32)\n",
        "            frame_buffer.append(input_array)\n",
        "\n",
        "            # Determine the weights based on the buffer size\n",
        "            if len(frame_buffer) == frame_buffer.maxlen:\n",
        "                normalized_weights = original_weights / np.sum(original_weights)\n",
        "            else:\n",
        "                # Use only the most recent weights corresponding to the number of frames available\n",
        "                recent_weights = original_weights[-len(frame_buffer):]\n",
        "                normalized_weights = recent_weights / np.sum(recent_weights)\n",
        "\n",
        "            # Compute the weighted average of images\n",
        "            blended_image_array = np.average(np.stack(frame_buffer, axis=0), axis=0, weights=normalized_weights)\n",
        "            blended_image = Image.fromarray(np.uint8(blended_image_array))\n",
        "\n",
        "            depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "            depth_gt_array = np.array(depth_gt_image)\n",
        "            depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "            depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(blended_image.size, Image.BILINEAR)\n",
        "            depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "            # Call your depth estimation pipeline on the blended image\n",
        "            pipeline_output = pipe(\n",
        "                blended_image,\n",
        "                denoising_steps=4,\n",
        "                ensemble_size=5,\n",
        "                processing_res=768,\n",
        "                match_input_res=True,\n",
        "                color_map=\"Spectral\",\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "            depth_pred = pipeline_output.depth_np\n",
        "\n",
        "            depth_buffer.append(depth_pred)\n",
        "\n",
        "            # RMSE on the latest depth map only\n",
        "            rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "            errors.append(rmse)\n",
        "            print(f'RMSE: {rmse}')\n",
        "\n",
        "    return np.mean(errors), errors"
      ],
      "metadata": {
        "id": "vrHMthMfdxAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_exponential_decay(rgb_depth_pairs):\n",
        "    depth_buffer = deque(maxlen=5)\n",
        "    errors = []\n",
        "    frame_buffer = deque(maxlen=5)\n",
        "    lambda_decay = 0.8\n",
        "\n",
        "    for rgb_path, depth_gt_path in tqdm(rgb_depth_pairs, desc=\"Processing images\"):\n",
        "        with torch.no_grad():\n",
        "              input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "              input_array = np.array(input_image, dtype=np.float32)\n",
        "              frame_buffer.append(input_array)\n",
        "\n",
        "              # Calculate weights based on buffer size\n",
        "              weights = np.array([lambda_decay**i for i in range(len(frame_buffer)-1, -1, -1)])\n",
        "              normalized_weights = weights / np.sum(weights)\n",
        "\n",
        "              # Compute weighted average of images\n",
        "              blended_image_array = np.average(np.stack(frame_buffer, axis=0), axis=0, weights=normalized_weights)\n",
        "              blended_image = Image.fromarray(np.uint8(blended_image_array))\n",
        "\n",
        "              depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "              depth_gt_array = np.array(depth_gt_image)\n",
        "              depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "              depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(blended_image.size, Image.BILINEAR)\n",
        "              depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "              # Call your depth estimation pipeline on the blended image\n",
        "              pipeline_output = pipe(\n",
        "                  blended_image,\n",
        "                  denoising_steps=4,\n",
        "                  ensemble_size=5,\n",
        "                  processing_res=768,\n",
        "                  match_input_res=True,\n",
        "                  color_map=\"Spectral\",\n",
        "                  show_progress_bar=True\n",
        "              )\n",
        "              depth_pred = pipeline_output.depth_np\n",
        "\n",
        "              depth_buffer.append(depth_pred)\n",
        "\n",
        "              # RMSE on the latest depth map only\n",
        "              rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "              errors.append(rmse)\n",
        "              print(f'RMSE: {rmse}')\n",
        "\n",
        "    return np.mean(errors), errors"
      ],
      "metadata": {
        "id": "MQ6InOtEeL3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_average(rgb_depth_pairs):\n",
        "    depth_buffer = deque(maxlen=5)\n",
        "    errors = []\n",
        "    frame_buffer = deque(maxlen=5)\n",
        "\n",
        "    for rgb_path, depth_gt_path in tqdm(rgb_depth_pairs, desc=\"Processing images\"):\n",
        "        with torch.no_grad():\n",
        "            input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "            input_array = np.array(input_image, dtype=np.float32)\n",
        "            frame_buffer.append(input_array)\n",
        "\n",
        "            # Calculate equal weights for all images in the buffer\n",
        "            if len(frame_buffer) > 0:\n",
        "                normalized_weights = np.ones(len(frame_buffer)) / len(frame_buffer)\n",
        "            else:\n",
        "                normalized_weights = np.array([])\n",
        "\n",
        "            # Compute average of images\n",
        "            blended_image_array = np.average(np.stack(frame_buffer, axis=0), axis=0, weights=normalized_weights)\n",
        "            blended_image = Image.fromarray(np.uint8(blended_image_array))\n",
        "\n",
        "            depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "            depth_gt_array = np.array(depth_gt_image)\n",
        "            depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "            depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(blended_image.size, Image.BILINEAR)\n",
        "            depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "            # Call your depth estimation pipeline on the blended image\n",
        "            pipeline_output = pipe(\n",
        "                blended_image,\n",
        "                denoising_steps=4,\n",
        "                ensemble_size=5,\n",
        "                processing_res=768,\n",
        "                match_input_res=True,\n",
        "                color_map=\"Spectral\",\n",
        "                show_progress_bar=True\n",
        "            )\n",
        "            depth_pred = pipeline_output.depth_np\n",
        "\n",
        "            depth_buffer.append(depth_pred)\n",
        "\n",
        "            # RMSE on the latest depth map only\n",
        "            rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "            errors.append(rmse)\n",
        "            print(f'RMSE: {rmse}')\n",
        "    return np.mean(errors), errors"
      ],
      "metadata": {
        "id": "iU_IQwCHe7hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_baseline(rgb_depth_pairs):\n",
        "    errors = []\n",
        "    for rgb_path, depth_gt_path in tqdm(rgb_depth_pairs, desc=\"Processing images\"):\n",
        "        with torch.no_grad():\n",
        "            input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "            depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "            depth_gt_array = np.array(depth_gt_image)\n",
        "            depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "            depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(input_image.size, Image.BILINEAR)\n",
        "            depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "            # Call your depth estimation pipeline\n",
        "            pipeline_output = pipe(\n",
        "                    input_image,\n",
        "                    denoising_steps=4,\n",
        "                    ensemble_size=5,\n",
        "                    processing_res=768,\n",
        "                    match_input_res=True,\n",
        "                    color_map=\"Spectral\",\n",
        "                    show_progress_bar=True\n",
        "            )\n",
        "            depth_pred = pipeline_output.depth_np\n",
        "\n",
        "            rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "            errors.append(rmse)\n",
        "            print(f'RMSE: {rmse}')\n",
        "    return np.mean(errors), errors"
      ],
      "metadata": {
        "id": "ZDzZx4i3fDP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(dataset_paths.keys())\n",
        "current_key = keys[0]\n",
        "\n",
        "# Process only the second video\n",
        "mean_rmse, errors = process_video_optimized(dataset_paths[current_key])\n",
        "plot_errors(errors, f'Running RMSE Across {current_key}')\n",
        "print(f\"Average RMSE for {current_key}: {mean_rmse}\")"
      ],
      "metadata": {
        "id": "IBiNbzIHfLpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"RMSE for {current_key}: {errors}\")\n",
        "print(current_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAmceqjvlTTn",
        "outputId": "ebc24a18-8c15-4ae8-e4b5-fa9093314ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE for sample_2: [0.3688975, 0.36967906, 0.30891892, 0.33703595, 0.34714332, 0.35215288, 0.3646447, 0.34880856, 0.35053036, 0.3452124, 0.35682526, 0.36253852, 0.3408827, 0.3533279, 0.34189105, 0.34139425, 0.33831543, 0.34103617, 0.34061489, 0.34724608, 0.3412142, 0.3329564, 0.3344652, 0.3415812, 0.3592501, 0.34997138, 0.34696892, 0.35890526, 0.34587604, 0.36945957, 0.3492622, 0.3307263, 0.32984254, 0.33561787, 0.3272429, 0.34261015, 0.35631803, 0.3292814, 0.33928084, 0.34562105, 0.34719127, 0.31634128, 0.34739035, 0.3520074, 0.33738714, 0.3329149, 0.33168855, 0.33326676, 0.3363141, 0.34884605, 0.32949933, 0.3333555, 0.33736798, 0.3481781, 0.3367785, 0.3414029, 0.30640128, 0.31506127, 0.33648556, 0.36096376, 0.32493883, 0.33809954, 0.3200681, 0.329629, 0.34908876, 0.3562902, 0.32989317, 0.31921762, 0.36628598, 0.3594858, 0.34200868, 0.36178297, 0.3584098, 0.3209798, 0.33532596, 0.35584727, 0.33648467, 0.33627844, 0.34045452, 0.33428434, 0.34827194, 0.3325955, 0.3495238, 0.3279648, 0.3218414, 0.34241572, 0.30606747, 0.3349974, 0.3511402, 0.34195113, 0.34279537, 0.35822266, 0.35658175, 0.35632652, 0.3575502, 0.37148565, 0.37871033, 0.3672313, 0.38060397, 0.36843824, 0.3717016, 0.3603748, 0.38376984, 0.39186475, 0.39653316, 0.35550028, 0.3639244, 0.3929473, 0.40009975, 0.36761388, 0.36963564, 0.3676629, 0.38668096, 0.3782193, 0.39464003, 0.3989216, 0.39315677, 0.3193396, 0.31896526, 0.31223425, 0.33527464, 0.34120685, 0.32017192, 0.3261058, 0.3592085, 0.335214, 0.35467714, 0.35102367, 0.35870838, 0.34717083, 0.31385556, 0.37811217, 0.32153884, 0.2751704, 0.29882511, 0.37974468, 0.3703897, 0.36946642, 0.36332434, 0.38582367, 0.3699581, 0.35568362, 0.34261465, 0.35457546, 0.38508442, 0.35658225, 0.35987407, 0.35459715, 0.35680383, 0.35828468, 0.35416454, 0.34124112, 0.34271201, 0.35094407, 0.33952782, 0.3108251, 0.28060704, 0.31416282, 0.314573, 0.32153028, 0.3202487, 0.36066872, 0.36386982, 0.36033362, 0.36132598, 0.37154192, 0.36855033, 0.35464916, 0.34926182, 0.33703843, 0.3442567, 0.34354, 0.3546186, 0.35259354, 0.34453005, 0.32702836, 0.3416261, 0.33456188, 0.32565933, 0.33947337, 0.36554986, 0.37223893, 0.35079274, 0.3869846, 0.3904199, 0.38684112, 0.39175865, 0.41463536, 0.3849336, 0.2843393, 0.2855721, 0.28254938]\n",
            "sample_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(dataset_paths.keys())\n",
        "current_key = keys[0]\n",
        "\n",
        "\n",
        "mean_rmse_base_1, errors_base_1 = process_video_baseline(dataset_paths[current_key])\n",
        "plot_errors(errors_base_1, f'Running RMSE Across {current_key}')\n",
        "print(f\"Average RMSE for {current_key}: {mean_rmse_base_1}\")\n",
        "print(f\"RMSE for {current_key}: {errors_base_1}\")"
      ],
      "metadata": {
        "id": "jPLVIJwslDqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average RMSE for {current_key}: {mean_rmse_base_1}\")\n",
        "print(f\"RMSE for {current_key}: {errors_base_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WwCalBn1MUW",
        "outputId": "94f76217-ff32-4d24-de51-9872ce5b6ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE for sample_2: 0.3507709503173828\n",
            "RMSE for sample_2: [0.37633568, 0.36740068, 0.37558514, 0.37047654, 0.3632187, 0.3709728, 0.35682863, 0.36476618, 0.36804646, 0.35948792, 0.35404092, 0.3601339, 0.34770527, 0.35543835, 0.34955055, 0.34321564, 0.34540886, 0.35107002, 0.35141963, 0.35417795, 0.35982662, 0.36105496, 0.35624057, 0.34340587, 0.3721493, 0.36884487, 0.36636475, 0.35299277, 0.3525359, 0.3580638, 0.3610465, 0.36061114, 0.36244515, 0.35645455, 0.3493328, 0.3432749, 0.34648037, 0.3570198, 0.33345327, 0.3435514, 0.33834785, 0.34554338, 0.34646392, 0.36042428, 0.3589725, 0.37231988, 0.37409192, 0.3492754, 0.35811773, 0.36068016, 0.35750026, 0.3657754, 0.35585278, 0.360988, 0.36458892, 0.36142832, 0.34967345, 0.3538665, 0.36086658, 0.36107618, 0.35462108, 0.3581254, 0.35027164, 0.36851376, 0.36163244, 0.36829343, 0.36417198, 0.3658908, 0.3568185, 0.36375442, 0.3746945, 0.37187123, 0.38424954, 0.367514, 0.35438952, 0.35714942, 0.35419384, 0.35362962, 0.3662249, 0.35445768, 0.36082166, 0.34599242, 0.34806657, 0.35347718, 0.34868962, 0.35601467, 0.35695314, 0.35566428, 0.35098428, 0.35616422, 0.3659563, 0.3645378, 0.36215776, 0.3709794, 0.3716763, 0.3666483, 0.38311696, 0.38654244, 0.38910538, 0.3823308, 0.38490734, 0.3630649, 0.3827536, 0.3794725, 0.37997055, 0.37468407, 0.39288157, 0.35490316, 0.3782844, 0.3854553, 0.3738237, 0.38074154, 0.3788883, 0.37513608, 0.38442454, 0.38623753, 0.39533406, 0.32674238, 0.32087418, 0.32416525, 0.3285335, 0.32201853, 0.31564808, 0.3226935, 0.32851487, 0.3271527, 0.31973737, 0.3227611, 0.3383492, 0.33983928, 0.22568054, 0.3350738, 0.25319767, 0.22970828, 0.22952762, 0.33992645, 0.34103608, 0.3522331, 0.346632, 0.35686523, 0.35380554, 0.3380976, 0.33747825, 0.34102595, 0.33800992, 0.32188922, 0.33294868, 0.3391103, 0.32073158, 0.3395671, 0.3161528, 0.34153837, 0.3116776, 0.33595455, 0.34535173, 0.2965486, 0.30818313, 0.2955851, 0.28859755, 0.30144376, 0.3053844, 0.347908, 0.36445576, 0.36197656, 0.364384, 0.3653251, 0.348162, 0.36274529, 0.35608587, 0.35311693, 0.35587963, 0.35163972, 0.35190505, 0.341479, 0.34105802, 0.3435163, 0.3477148, 0.35942385, 0.37772495, 0.36008787, 0.36482424, 0.36357644, 0.35555744, 0.34818995, 0.3526637, 0.36184102, 0.356821, 0.3796176, 0.37453687, 0.26828715, 0.28476414, 0.30317315]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(dataset_paths.keys())\n",
        "current_key = keys[0]\n",
        "\n",
        "\n",
        "mean_rmse_base_2, errors_base_2 = process_video_exponential_decay(dataset_paths[current_key])\n",
        "plot_errors(errors_base_2, f'Running RMSE Across {current_key}')\n",
        "print(f\"Average RMSE for {current_key}: {mean_rmse_base_2}\")\n",
        "print(f\"RMSE for {current_key}: {errors_base_2}\")"
      ],
      "metadata": {
        "id": "LUs5nNQTxXJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average RMSE for {current_key}: {mean_rmse_base_2}\")\n",
        "print(f\"RMSE for {current_key}: {errors_base_2}\")"
      ],
      "metadata": {
        "id": "STOU-rmR7NhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(dataset_paths.keys())\n",
        "current_key = 'sample_2'\n",
        "\n",
        "\n",
        "mean_rmse_base_3, errors_base_3 = process_video_average(dataset_paths[current_key])\n",
        "plot_errors(errors_base_3, f'Running RMSE Across {current_key}')\n",
        "print(f\"Average RMSE for {current_key}: {mean_rmse_base_3}\")\n",
        "print(f\"RMSE for {current_key}: {errors_base_3}\")"
      ],
      "metadata": {
        "id": "u99qFSLQI2Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on data with weights as trained\n",
        "depth_buffer = deque(maxlen=5)\n",
        "frame_buffer = deque(maxlen=5)\n",
        "errors = []\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "original_weights = np.array([0.2854117, 0.04321782, -0.33442682, 0.27889505, 0.7269023])\n",
        "\n",
        "for rgb_path, depth_gt_path in tqdm(dataset, total=len(dataset), desc=\"Processing images\"):\n",
        "    with torch.no_grad():\n",
        "        input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "        input_array = np.array(input_image, dtype=np.float32)\n",
        "        frame_buffer.append(input_array)\n",
        "\n",
        "        # Determine the weights based on the buffer size\n",
        "        if len(frame_buffer) == frame_buffer.maxlen:\n",
        "            normalized_weights = original_weights / np.sum(original_weights)\n",
        "        else:\n",
        "            # Use only the most recent weights corresponding to the number of frames available\n",
        "            recent_weights = original_weights[-len(frame_buffer):]\n",
        "            normalized_weights = recent_weights / np.sum(recent_weights)\n",
        "\n",
        "        # Compute the weighted average of images\n",
        "        blended_image_array = np.average(np.stack(frame_buffer, axis=0), axis=0, weights=normalized_weights)\n",
        "        blended_image = Image.fromarray(np.uint8(blended_image_array))\n",
        "\n",
        "        depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "        depth_gt_array = np.array(depth_gt_image)\n",
        "        depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "        depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(blended_image.size, Image.BILINEAR)\n",
        "        depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "        # Call your depth estimation pipeline on the blended image\n",
        "        pipeline_output = pipe(\n",
        "            blended_image,\n",
        "            denoising_steps=4,\n",
        "            ensemble_size=5,\n",
        "            processing_res=768,\n",
        "            match_input_res=True,\n",
        "            color_map=\"Spectral\",\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        depth_pred = pipeline_output.depth_np\n",
        "\n",
        "        depth_buffer.append(depth_pred)\n",
        "\n",
        "        # RMSE on the latest depth map only\n",
        "        rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "        errors.append(rmse)\n",
        "        print(f'RMSE: {rmse}')\n",
        "\n",
        "# Finalize plot\n",
        "ax.plot(errors, label='RMSE per Image')\n",
        "ax.set_xlabel('Image Index')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.set_title('Running RMSE Across Dataset')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"dataset_rmse_graph.pdf\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"Average RMSE for the dataset: {np.mean(errors)}\")"
      ],
      "metadata": {
        "id": "746ELW44Xm_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on data with exponentially decaying weights\n",
        "depth_buffer = deque(maxlen=5)\n",
        "frame_buffer = deque(maxlen=5)\n",
        "errors = []\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Lambda for exponential decay\n",
        "lambda_decay = 0.8\n",
        "\n",
        "for rgb_path, depth_gt_path in tqdm(dataset, total=len(dataset), desc=\"Processing images\"):\n",
        "    with torch.no_grad():\n",
        "        input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "        input_array = np.array(input_image, dtype=np.float32)\n",
        "        frame_buffer.append(input_array)\n",
        "\n",
        "        # Calculate weights based on buffer size\n",
        "        weights = np.array([lambda_decay**i for i in range(len(frame_buffer)-1, -1, -1)])\n",
        "        normalized_weights = weights / np.sum(weights)\n",
        "\n",
        "        # Compute weighted average of images\n",
        "        blended_image_array = np.average(np.stack(frame_buffer, axis=0), axis=0, weights=normalized_weights)\n",
        "        blended_image = Image.fromarray(np.uint8(blended_image_array))\n",
        "\n",
        "        depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "        depth_gt_array = np.array(depth_gt_image)\n",
        "        depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "        depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(blended_image.size, Image.BILINEAR)\n",
        "        depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "        # Call your depth estimation pipeline on the blended image\n",
        "        pipeline_output = pipe(\n",
        "            blended_image,\n",
        "            denoising_steps=4,\n",
        "            ensemble_size=5,\n",
        "            processing_res=768,\n",
        "            match_input_res=True,\n",
        "            color_map=\"Spectral\",\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        depth_pred = pipeline_output.depth_np\n",
        "\n",
        "        depth_buffer.append(depth_pred)\n",
        "\n",
        "        # RMSE on the latest depth map only\n",
        "        rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "        errors.append(rmse)\n",
        "        print(f'RMSE: {rmse}')\n",
        "\n",
        "# Finalize plot\n",
        "ax.plot(errors, label='RMSE per Image')\n",
        "ax.set_xlabel('Image Index')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.set_title('Running RMSE Across Dataset')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"dataset_rmse_graph.pdf\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"Average RMSE for the dataset: {np.mean(errors)}\")"
      ],
      "metadata": {
        "id": "Mp0GYBOIXpcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on data with equal weights throughout (just average)\n",
        "depth_buffer = deque(maxlen=5)\n",
        "frame_buffer = deque(maxlen=5)\n",
        "errors = []\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for rgb_path, depth_gt_path in tqdm(dataset, total=len(dataset), desc=\"Processing images\"):\n",
        "    with torch.no_grad():\n",
        "        input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "        input_array = np.array(input_image, dtype=np.float32)\n",
        "        frame_buffer.append(input_array)\n",
        "\n",
        "        # Calculate equal weights for all images in the buffer\n",
        "        if len(frame_buffer) > 0:\n",
        "            normalized_weights = np.ones(len(frame_buffer)) / len(frame_buffer)\n",
        "        else:\n",
        "            normalized_weights = np.array([])\n",
        "\n",
        "        # Compute average of images\n",
        "        blended_image_array = np.average(np.stack(frame_buffer, axis=0), axis=0, weights=normalized_weights)\n",
        "        blended_image = Image.fromarray(np.uint8(blended_image_array))\n",
        "\n",
        "        depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "        depth_gt_array = np.array(depth_gt_image)\n",
        "        depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "        depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(blended_image.size, Image.BILINEAR)\n",
        "        depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "        # Call your depth estimation pipeline on the blended image\n",
        "        pipeline_output = pipe(\n",
        "            blended_image,\n",
        "            denoising_steps=4,\n",
        "            ensemble_size=5,\n",
        "            processing_res=768,\n",
        "            match_input_res=True,\n",
        "            color_map=\"Spectral\",\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        depth_pred = pipeline_output.depth_np\n",
        "\n",
        "        depth_buffer.append(depth_pred)\n",
        "\n",
        "        # RMSE on the latest depth map only\n",
        "        rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "        errors.append(rmse)\n",
        "        print(f'RMSE: {rmse}')\n",
        "\n",
        "# Finalize plot\n",
        "ax.plot(errors, label='RMSE per Image')\n",
        "ax.set_xlabel('Image Index')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.set_title('Running RMSE Across Dataset')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"dataset_rmse_graph.pdf\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"Average RMSE for the dataset: {np.mean(errors)}\")"
      ],
      "metadata": {
        "id": "m5f6hq9NXsbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on data without any conditional inputs (baseline)\n",
        "errors = []\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for rgb_path, depth_gt_path in tqdm(dataset, total=len(dataset), desc=\"Processing images\"):\n",
        "    with torch.no_grad():\n",
        "        input_image = Image.open(rgb_path).convert(\"RGB\")\n",
        "        depth_gt_image = Image.open(depth_gt_path).convert(\"RGB\")\n",
        "        depth_gt_array = np.array(depth_gt_image)\n",
        "        depth_gt_decoded = depth_gt_array.mean(axis=2).astype(np.float32) / 255.0\n",
        "        depth_gt_resized = Image.fromarray(depth_gt_decoded).resize(input_image.size, Image.BILINEAR)\n",
        "        depth_gt_final = np.array(depth_gt_resized)\n",
        "\n",
        "        # Call your depth estimation pipeline\n",
        "        pipeline_output = pipe(\n",
        "                input_image,\n",
        "                denoising_steps=4,\n",
        "                ensemble_size=5,\n",
        "                processing_res=768,\n",
        "                match_input_res=True,\n",
        "                color_map=\"Spectral\",\n",
        "                show_progress_bar=True\n",
        "        )\n",
        "        depth_pred = pipeline_output.depth_np\n",
        "\n",
        "        rmse = calculate_rmse(depth_pred, depth_gt_final)\n",
        "        errors.append(rmse)\n",
        "        print(f'RMSE: {rmse}')\n",
        "\n",
        "# Finalize plot\n",
        "ax.plot(errors, label='RMSE per Image')\n",
        "ax.set_xlabel('Image Index')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.set_title('Running RMSE Across Dataset')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"dataset_rmse_graph.pdf\")\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"Average RMSE for the dataset: {np.mean(errors)}\")"
      ],
      "metadata": {
        "id": "6l79mfzkYsVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}